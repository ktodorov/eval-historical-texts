{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'french'\n",
    "\n",
    "sys.argv = [\n",
    "\"--device cuda\",\n",
    "\"--data-folder\", \"..\\\\data\",\n",
    "\"--seed\", \"13\",\n",
    "\"--configuration\", \"rnn-simple\",\n",
    "\"--language\", language,\n",
    "\"--challenge\", \"named-entity-recognition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure container:\n",
    "from dependency_injection.ioc_container import IocContainer\n",
    "\n",
    "container = IocContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = 'eval-historical-texts'\n",
    "project = 'named-entity-recognition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wandb_runs():\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(path=f'{entity}/{project}', filters={\n",
    "        'createdAt': {\n",
    "            '$gt': '20200621000000'\n",
    "        },\n",
    "        'config.language': language,\n",
    "        'state': {\n",
    "            '$ne': 'running'\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded 240 runs\n"
    }
   ],
   "source": [
    "runs = get_wandb_runs()\n",
    "print(f'Loaded {len(runs)} runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Updating... (16/240)\nUpdating configuration...\nUpdating summary...\n"
    }
   ],
   "source": [
    "def get_summary_value(run, key: str):\n",
    "    if key not in run.summary.keys():\n",
    "        return None\n",
    "\n",
    "    return run.summary[key]\n",
    "\n",
    "pretrained_model_type_key = 'pretrained_model_type'\n",
    "fine_tune_key = 'fine_tune'\n",
    "\n",
    "for i, run in enumerate(runs):\n",
    "    if 'pretrained_model_type' in run.config.keys():\n",
    "        continue\n",
    " \n",
    "    clear_output(wait=True)\n",
    "    print(f'Updating... ({i}/{len(runs)})')\n",
    "    if len(run.config.keys()) == 0:# or fine_tune_key in run.config.keys() or pretrained_model_type_key in run.config.keys():\n",
    "        continue\n",
    "\n",
    "    if 'pretrained_model_type' in run.config.keys():\n",
    "        continue\n",
    "\n",
    "    if 'pretrained_model_type' not in run.config.keys():\n",
    "        include_pretrained_model = run.config['include_pretrained_model']\n",
    "        include_fasttext_model = run.config['include_fasttext_model']\n",
    "        fine_tune_pretrained = run.config['fine_tune_pretrained']\n",
    "        fine_tune_after_convergence = run.config['fine_tune_after_convergence']\n",
    "\n",
    "        pretr_type = 'none'\n",
    "        if include_pretrained_model and include_fasttext_model:\n",
    "            pretr_type = 'both'\n",
    "        elif include_pretrained_model:\n",
    "            pretr_type = 'bert'\n",
    "        elif include_fasttext_model:\n",
    "            pretr_type = 'fast-text'\n",
    "\n",
    "        fine_tune = (fine_tune_pretrained or fine_tune_after_convergence)\n",
    "        \n",
    "        print('Updating configuration...')\n",
    "        run.config.update({\n",
    "            'pretrained_model_type': pretr_type,\n",
    "            'fine_tune': fine_tune\n",
    "        })\n",
    "\n",
    "\n",
    "    all_scores = [\n",
    "        get_summary_value(run, 'Best - f1-score-micro-partial-all-component'),\n",
    "        get_summary_value(run, 'Best - f1-score-micro-partial-all-literal-coarse'),\n",
    "        get_summary_value(run, 'Best - f1-score-micro-partial-all-literal-fine'),\n",
    "        get_summary_value(run, 'Best - f1-score-micro-partial-all-metonymic-coarse'),\n",
    "        get_summary_value(run, 'Best - f1-score-micro-partial-all-metonymic-fine'),\n",
    "        get_summary_value(run, 'Best - f1-score-micro-partial-all-nested')\n",
    "    ]\n",
    "\n",
    "    all_scores = [x for x in all_scores if x is not None]\n",
    "    avg_score = np.mean(all_scores)\n",
    "        \n",
    "    print('Updating summary...')\n",
    "    run.summary.update({\n",
    "        'average_score': avg_score\n",
    "    })\n",
    "\n",
    "    run.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bitevalenvcondab07c5918277c4c33a244293f5160293b",
   "display_name": "Python 3.7.5 64-bit ('eval-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}