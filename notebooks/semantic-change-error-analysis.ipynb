{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "\"--device cuda\",\n",
    "\"--seed\", \"13\",\n",
    "\"--configuration\", \"kbert\",\n",
    "\"--language\", \"swedish\",\n",
    "\"--challenge\", \"semantic-change\",\n",
    "\"--evaluation-type\", \"cosine-distance\", \"euclidean-distance\",\n",
    "\"--pretrained-weights\", \"bert-base-multilingual-cased\",\n",
    "\"--pretrained-max-length\", \"512\",\n",
    "\"--experiment-types\", \"word-similarity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure container:\n",
    "from dependency_injection.ioc_container import IocContainer\n",
    "\n",
    "container = IocContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_service = container.file_service()\n",
    "arguments_service = container.arguments_service()\n",
    "metrics_service = container.metrics_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_language_text(corpus: int):\n",
    "    data_path = os.path.join('..', file_service.get_challenge_path(), 'eval', str(arguments_service.language), f'corpus{corpus}')\n",
    "    for dir_name in os.listdir(data_path):\n",
    "        dir_path = os.path.join(data_path, dir_name)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if not file_name.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            with open(os.path.join(dir_path, file_name), 'r', encoding='utf-8') as language_file:\n",
    "                language_text = language_file.read().replace('\\n', ' \\n ').lower().split(' ')\n",
    "                language_text = list(filter(None, language_text))\n",
    "                return language_text\n",
    "            \n",
    "def get_target_words():\n",
    "    data_path = os.path.join('..', file_service.get_challenge_path(), 'eval', str(arguments_service.language), 'targets.txt')\n",
    "    with open(data_path, 'r', encoding='utf-8') as targets_file:\n",
    "        targets = [x.replace('\\n', '').lower() for x in targets_file.readlines() if x != '']\n",
    "        return targets\n",
    "    \n",
    "def indices_of_targets(list_to_check, targets):\n",
    "    result = [(x,i) for i, x in enumerate(list_to_check) if x in targets]\n",
    "    return result\n",
    "\n",
    "def get_word_contexts(target_words, all_tokens):\n",
    "    close_word_contexts = {}\n",
    "    far_word_contexts = {}\n",
    "    all_tokens_length = len(all_tokens)\n",
    "    min_threshold = 0\n",
    "    window_sizes = list(range(1, 11))\n",
    "    window_size_threshold = 5\n",
    "    \n",
    "    target_indices = indices_of_targets(all_tokens, target_words)\n",
    "    for target_word in target_words:\n",
    "        current_word_close_context = Counter()\n",
    "        current_word_far_context = Counter()\n",
    "        indices = [x[1] for x in target_indices if x[0] == target_word]\n",
    "        for index in indices:\n",
    "            back_window_depleted = False\n",
    "            forward_window_depleted = False\n",
    "            for window_size in window_sizes:\n",
    "                if index - window_size >= 0 and not back_window_depleted:\n",
    "                    if all_tokens[index-window_size] == '\\n':\n",
    "                        back_window_depleted = True\n",
    "                        continue\n",
    "                        \n",
    "                    if window_size <= window_size_threshold:\n",
    "                        current_word_close_context.update({all_tokens[index-window_size] : 1})\n",
    "                    else:\n",
    "                        current_word_far_context.update({all_tokens[index-window_size] : 1})\n",
    "                        \n",
    "                if index + window_size < all_tokens_length and not forward_window_depleted:\n",
    "                    if all_tokens[index+window_size] == '\\n':\n",
    "                        forward_window_depleted = True\n",
    "                        continue\n",
    "                        \n",
    "                    if window_size <= window_size_threshold:\n",
    "                        current_word_close_context.update({all_tokens[index+window_size] : 1})\n",
    "                    else:\n",
    "                        current_word_far_context.update({all_tokens[index+window_size] : 1})\n",
    "        \n",
    "        current_word_close_context = Counter({x : current_word_close_context[x] for x in current_word_close_context if current_word_close_context[x] >= min_threshold})\n",
    "        current_word_far_context = Counter({x : current_word_far_context[x] for x in current_word_far_context if current_word_far_context[x] >= min_threshold})\n",
    "        close_word_contexts[target_word] = current_word_close_context\n",
    "        far_word_contexts[target_word] = current_word_far_context\n",
    "    \n",
    "    return close_word_contexts, far_word_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tokens_1 = read_language_text(corpus=1)\n",
    "language_tokens_2 = read_language_text(corpus=2)\n",
    "\n",
    "target_words = get_target_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close_word_contexts_1, far_word_contexts_1 = get_word_contexts(target_words, language_tokens_1)\n",
    "# close_word_contexts_2, far_word_contexts_2 = get_word_contexts(target_words, language_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close_jaccard_similarities = {}\n",
    "# far_jaccard_similarities = {}\n",
    "\n",
    "# close_cosine_distances = {}\n",
    "# far_cosine_distances = {}\n",
    "\n",
    "# target_words.sort(key=lambda x: x.upper())\n",
    "# for target_word in target_words:\n",
    "#     close_context_words_1 = list(close_word_contexts_1[target_word].elements())\n",
    "#     close_context_words_1.sort(key=lambda x: x.upper())\n",
    "    \n",
    "#     close_context_words_2 = list(close_word_contexts_2[target_word].elements())\n",
    "#     close_context_words_2.sort(key=lambda x: x.upper())\n",
    "    \n",
    "#     close_jaccard_similarity = metrics_service.calculate_jaccard_similarity(close_context_words_1, close_context_words_2)\n",
    "#     if not close_jaccard_similarity:\n",
    "#         raise Exception(f'nan close similarity for {target_word}')\n",
    "#     close_jaccard_similarities[target_word] = close_jaccard_similarity\n",
    "    \n",
    "#     close_context_words = list(set(list(close_word_contexts_1[target_word].keys()) + list(close_word_contexts_2[target_word].keys())))\n",
    "#     new_close_context_words_1 = [close_word_contexts_1[target_word][key] for key in close_context_words]\n",
    "#     new_close_context_words_2 = [close_word_contexts_2[target_word][key] for key in close_context_words]\n",
    "    \n",
    "#     close_cosine_distance = metrics_service.calculate_cosine_distance(new_close_context_words_1, new_close_context_words_2)\n",
    "#     if not close_cosine_distance:\n",
    "#         raise Exception(f'nan close distance for {target_word}')\n",
    "#     close_cosine_distances[target_word] = close_cosine_distance\n",
    "    \n",
    "#     far_context_words_1 = list(far_word_contexts_1[target_word].elements())\n",
    "#     far_context_words_1.sort()\n",
    "    \n",
    "#     far_context_words_2 = list(far_word_contexts_2[target_word].elements())\n",
    "#     far_context_words_2.sort()\n",
    "    \n",
    "#     far_jaccard_similarity = metrics_service.calculate_jaccard_similarity(far_context_words_1, far_context_words_2)\n",
    "#     far_jaccard_similarities[target_word] = far_jaccard_similarity\n",
    "    \n",
    "#     far_context_words = list(set(list(far_word_contexts_1[target_word].keys()) + list(far_word_contexts_2[target_word].keys())))\n",
    "#     new_far_context_words_1 = [far_word_contexts_1[target_word][key] for key in far_context_words]\n",
    "#     new_far_context_words_2 = [far_word_contexts_2[target_word][key] for key in far_context_words]\n",
    "    \n",
    "#     far_cosine_distance = metrics_service.calculate_cosine_distance(new_far_context_words_1, new_far_context_words_2)\n",
    "#     far_cosine_distances[target_word] = far_cosine_distance\n",
    "\n",
    "   \n",
    "# print('\\n'.join([str(x) for x in list(close_jaccard_similarities.values())]))\n",
    "# print('---')\n",
    "# print('\\n'.join([str(x) for x in list(far_jaccard_similarities.values())]))\n",
    "# print('---')\n",
    "# print('\\n'.join([str(x) for x in list(close_cosine_distances.values())]))\n",
    "# print('---')\n",
    "# print('\\n'.join([str(x) for x in list(far_cosine_distances.values())]))\n",
    "# print('---')\n",
    "\n",
    "# # print(far_jaccard_similarities)\n",
    "# # print(close_cosine_distances)\n",
    "# # print(far_cosine_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('eval-env': conda)",
   "language": "python",
   "name": "python37564bitevalenvcondab07c5918277c4c33a244293f5160293b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
