{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from collections import Counter, OrderedDict\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "from enums.ner_type import NERType\n",
    "import itertools\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'french'\n",
    "\n",
    "sys.argv = [\n",
    "\"--device cuda\",\n",
    "\"--data-folder\", \"..\\\\data\",\n",
    "\"--seed\", \"13\",\n",
    "\"--configuration\", \"rnn-simple\",\n",
    "\"--language\", language,\n",
    "\"--challenge\", \"named-entity-recognition\",\n",
    "\"--label-type\", \"coarse\",\n",
    "\"--experiment-types\", \"word-similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure container:\n",
    "from dependency_injection.ioc_container import IocContainer\n",
    "\n",
    "container = IocContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_service = container.file_service()\n",
    "arguments_service = container.arguments_service()\n",
    "metrics_service = container.metrics_service()\n",
    "process_service = container.process_service()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenInfo:\n",
    "    def __init__(\n",
    "        self,\n",
    "        token: str,\n",
    "        literal_coarse_tag: str=None,\n",
    "        literal_fine_tag: str=None,\n",
    "        metonymic_coarse_tag: str=None,\n",
    "        metonymic_fine_tag: str=None,\n",
    "        content_tag: str=None,\n",
    "        nested_tag: str=None):\n",
    "        \n",
    "        self.token = token\n",
    "        self.literal_tags: Dict[NERType, str] = { NERType.Coarse : literal_coarse_tag, NERType.Fine : literal_fine_tag }\n",
    "        self.metonymic_tags: Dict[NERType, str] = { NERType.Coarse : metonymic_coarse_tag, NERType.Fine : metonymic_fine_tag }\n",
    "        self.content_tag: str = content_tag\n",
    "        self.nested_tag: str = nested_tag\n",
    "\n",
    "class ModelType(Enum):\n",
    "    GroundTruth = 0\n",
    "    Baseline = 1\n",
    "    Ours = 2\n",
    "\n",
    "class NERDataStatistics:\n",
    "    def __init__(self):\n",
    "        self.content_tags = Counter()\n",
    "        self.nested_entity_tags = Counter()\n",
    "        self.literal_entity_tags: Dict[NERType, Counter] = {\n",
    "            NERType.Coarse: Counter(),\n",
    "            NERType.Fine: Counter(),\n",
    "        }\n",
    "        \n",
    "        self.metonymic_entity_tags: Dict[NERType, Counter] = {\n",
    "            NERType.Coarse: Counter(),\n",
    "            NERType.Fine: Counter(),\n",
    "        }\n",
    "\n",
    "    def add_row_data(self, row):\n",
    "        self.nested_entity_tags[row['NE-NESTED']] += 1\n",
    "        self.content_tags[row['NE-FINE-COMP']] += 1\n",
    "\n",
    "        self.literal_entity_tags[NERType.Coarse][row['NE-COARSE-LIT']] += 1\n",
    "        self.literal_entity_tags[NERType.Fine][row['NE-FINE-LIT']] += 1\n",
    "\n",
    "        self.metonymic_entity_tags[NERType.Coarse][row['NE-COARSE-METO']] += 1\n",
    "        self.metonymic_entity_tags[NERType.Fine][row['NE-FINE-METO']] += 1\n",
    "\n",
    "    def print_data(self):\n",
    "        print('----------------')\n",
    "        print('Statistics data:')\n",
    "        print('----------------')\n",
    "        self._print_collection_data(self.nested_entity_tags, 'unique nested entity tags')\n",
    "        self._print_collection_data(self.content_tags, 'unique content tags')\n",
    "        self._print_collection_data(self.literal_entity_tags[NERType.Fine], 'unique fine literal entity tags')\n",
    "        self._print_collection_data(self.literal_entity_tags[NERType.Coarse], 'unique coarse literal entity tags')\n",
    "        self._print_collection_data(self.metonymic_entity_tags[NERType.Fine], 'unique fine metonymic entity tags')\n",
    "        self._print_collection_data(self.metonymic_entity_tags[NERType.Coarse], 'unique coarse metonymic entity tags')\n",
    "\n",
    "    def _get_none_percentage(self, collection) -> float:\n",
    "        result = (collection[\"O\"] / sum(collection.values())) * 100\n",
    "        return result\n",
    "    \n",
    "    def _print_collection_data(self, collection: Counter, name: str):\n",
    "        print(f'{name}: {len(collection)} || None percentage: {self._get_none_percentage(collection)}')\n",
    "\n",
    "def read_output_files(coarse_path, fine_path=None, print_stats=False):\n",
    "    result = []\n",
    "    statistics = NERDataStatistics()\n",
    "    with open(coarse_path, 'r', encoding='utf-8') as coarse_file:\n",
    "        reader = csv.DictReader(coarse_file, dialect=csv.excel_tab)\n",
    "        for i, row in enumerate(reader):\n",
    "            if row['TOKEN'].startswith('#'):\n",
    "                continue\n",
    "                \n",
    "            statistics.add_row_data(row)\n",
    "            token_info = TokenInfo(\n",
    "                token=row['TOKEN'],\n",
    "                literal_coarse_tag=row['NE-COARSE-LIT'].lower(),\n",
    "                literal_fine_tag=row['NE-FINE-LIT'].lower())\n",
    "\n",
    "            result.append(token_info)\n",
    "\n",
    "    if fine_path is not None:\n",
    "        with open(fine_path, 'r', encoding='utf-8') as fine_file:\n",
    "            reader = csv.DictReader(fine_file, dialect=csv.excel_tab)\n",
    "            counter = 0\n",
    "            for i, row in enumerate(reader):\n",
    "                if row['TOKEN'].startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                statistics.add_row_data(row)\n",
    "\n",
    "                result[counter].literal_tags[NERType.Fine] = row['NE-FINE-LIT'].lower()\n",
    "                counter += 1\n",
    "                \n",
    "    if print_stats:\n",
    "        statistics.print_data()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_path = os.path.join('..', '..', '..', 'challenges', 'clef', 'scorer', 'CLEF-HIPE-2020-scorer')\n",
    "\n",
    "dev_data_path = os.path.join(scorer_path, 'data', language, f'HIPE-data-v1.0-dev-{process_service.get_language_suffix(arguments_service.language)}.tsv')\n",
    "train_data_path = os.path.join(scorer_path, 'data', language, f'HIPE-data-v1.0-train-{process_service.get_language_suffix(arguments_service.language)}.tsv')\n",
    "baseline_coarse_path = os.path.join(scorer_path, 'output-baseline-coarse.tsv')\n",
    "baseline_fine_path = os.path.join(scorer_path, 'output-baseline-fine.tsv')\n",
    "model_coarse_path = os.path.join(scorer_path, 'output-french-coarse-pretr.tsv')\n",
    "model_fine_path = os.path.join(scorer_path, 'output-french-fine-pretr.tsv')\n",
    "\n",
    "# _ = read_output_files(train_data_path, dev_data_path, print_stats=True)\n",
    "\n",
    "truth_tokens = read_output_files(dev_data_path)\n",
    "baseline_tokens = read_output_files(baseline_coarse_path, baseline_fine_path)\n",
    "new_tokens = read_output_files(model_coarse_path, model_fine_path)\n",
    "\n",
    "assert len(baseline_tokens) == len(truth_tokens)\n",
    "assert len(baseline_tokens) == len(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelsDifference:\n",
    "    def __init__(self):\n",
    "        self.ground_truth = List[TokenInfo]\n",
    "        self.tokens_by_model: Dict[ModelType, List[TokenInfo]] = {}\n",
    "        self.difference_set: Dict[str, Counter] = {}\n",
    "\n",
    "    def add_token_set(self, model_type: ModelType, tokens: List[TokenInfo]):\n",
    "        if model_type == ModelType.GroundTruth:\n",
    "            self.ground_truth = tokens\n",
    "            return\n",
    "\n",
    "        if model_type in self.tokens_by_model.keys():\n",
    "            raise Exception(\"This model type is already added\")\n",
    "\n",
    "        self.tokens_by_model[model_type] = tokens\n",
    "\n",
    "    def calculate_difference_set(self):\n",
    "        self.difference_set = {}\n",
    "        for i, truth_token_info in enumerate(self.ground_truth):\n",
    "            self._calculate_difference_set_per_column(i, truth_token_info, lambda token_info: token_info.literal_tags[NERType.Coarse], 'LITERAL|COARSE')\n",
    "            self._calculate_difference_set_per_column(i, truth_token_info, lambda token_info: token_info.literal_tags[NERType.Fine], 'LITERAL|FINE')\n",
    "            self._calculate_difference_set_per_column(i, truth_token_info, lambda token_info: token_info.metonymic_tags[NERType.Coarse], 'METONYMICAL|FINE')\n",
    "            self._calculate_difference_set_per_column(i, truth_token_info, lambda token_info: token_info.metonymic_tags[NERType.Fine], 'METONYMICAL|FINE')\n",
    "            self._calculate_difference_set_per_column(i, truth_token_info, lambda token_info: token_info.content_tag, 'CONTENT')\n",
    "            self._calculate_difference_set_per_column(i, truth_token_info, lambda token_info: token_info.nested_tag, 'NESTED')\n",
    "            \n",
    "            \n",
    "    def _calculate_difference_set_per_column(self, i: int, truth_token_info: TokenInfo, column_extract: Callable, column_value: str):\n",
    "        predicted_values_by_model = { model_type: column_extract(tokens[i]) for model_type, tokens in self.tokens_by_model.items() }\n",
    "        difference_key = self._get_difference_key(column_extract(truth_token_info), predicted_values_by_model, column_value)\n",
    "        if difference_key not in self.difference_set.keys():\n",
    "            self.difference_set[difference_key] = Counter()\n",
    "\n",
    "        self.difference_set[difference_key][truth_token_info.token] += 1\n",
    "\n",
    "    def _get_difference_key(\n",
    "        self,\n",
    "        truth_value: str,\n",
    "        predicted_values_by_model: Dict[ModelType, str],\n",
    "        column_value: str) -> str:\n",
    "        results_dict = {}\n",
    "        for model_type, predicted_value in predicted_values_by_model.items():\n",
    "            results_dict[model_type] = (predicted_value == truth_value)\n",
    "\n",
    "        result = self._get_difference_key_by_bool(results_dict, column_value)\n",
    "        return result\n",
    "        \n",
    "    def _get_difference_key_by_bool(\n",
    "        self,\n",
    "        desired_prediction_status: Dict[ModelType, bool],\n",
    "        column_value: str) -> str:\n",
    "        results_dict = OrderedDict(sorted(desired_prediction_status.items(), key=lambda x: x[0].value))\n",
    "        result = '|'.join([f'{key.value}-{int(value)}' for key, value in results_dict.items()])\n",
    "        result += f'|{column_value}'\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_tokens_per_result(self, desired_prediction_status: Dict[ModelType, bool], column_value: str) -> Counter:\n",
    "        key = self._get_difference_key_by_bool(desired_prediction_status, column_value)\n",
    "        if key not in self.difference_set.keys():\n",
    "            return Counter()\n",
    "\n",
    "        return self.difference_set[key]\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_difference = ModelsDifference()\n",
    "models_difference.add_token_set(ModelType.GroundTruth, truth_tokens)\n",
    "models_difference.add_token_set(ModelType.Baseline, baseline_tokens)\n",
    "models_difference.add_token_set(ModelType.Ours, new_tokens)\n",
    "\n",
    "models_difference.calculate_difference_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "------------------\ncorrect by baseline, correct by ours [LITERAL|COARSE]\n\tTotal amount: 27254\n\tTop 10 most common: [(',', 1407), ('.', 1360), (\"'\", 1285), ('de', 1035), ('la', 619), ('à', 509), ('l', 452), ('et', 444), ('le', 414), ('les', 373)]\n------------------\ncorrect by baseline, wrong by ours [LITERAL|COARSE]\n\tTotal amount: 970\n\tTop 10 most common: [('de', 26), (',', 26), ('-', 23), ('.', 17), ('Conseil', 12), (\"'\", 12), ('fédéral', 11), ('des', 9), ('York', 9), ('Havas', 8)]\n------------------\nwrong by baseline, correct by ours [LITERAL|COARSE]\n\tTotal amount: 669\n\tTop 10 most common: [('.', 62), (',', 30), ('de', 29), (\"'\", 28), ('-', 27), ('\"', 14), ('à', 13), ('_', 12), ('M', 8), ('des', 8)]\n------------------\nwrong by baseline, wrong by ours [LITERAL|COARSE]\n\tTotal amount: 960\n\tTop 10 most common: [('de', 50), (',', 38), ('-', 34), ('.', 25), (\"'\", 16), ('M', 12), ('l', 10), ('la', 9), ('du', 8), ('et', 8)]\n------------------\ncorrect by baseline, correct by ours [LITERAL|FINE]\n\tTotal amount: 27270\n\tTop 10 most common: [(',', 1407), ('.', 1361), (\"'\", 1291), ('de', 1025), ('la', 620), ('à', 513), ('l', 452), ('et', 446), ('le', 412), ('les', 372)]\n------------------\ncorrect by baseline, wrong by ours [LITERAL|FINE]\n\tTotal amount: 934\n\tTop 10 most common: [('de', 36), ('-', 30), (',', 22), ('.', 18), ('Conseil', 13), ('fédéral', 12), (\"'\", 10), ('York', 9), ('France', 9), ('Esp', 9)]\n------------------\nwrong by baseline, correct by ours [LITERAL|FINE]\n\tTotal amount: 601\n\tTop 10 most common: [('.', 53), (',', 31), ('de', 26), ('-', 24), (\"'\", 20), ('_', 13), ('M', 12), ('\"', 11), ('à', 9), ('le', 7)]\n------------------\nwrong by baseline, wrong by ours [LITERAL|FINE]\n\tTotal amount: 1048\n\tTop 10 most common: [('de', 53), (',', 41), ('-', 34), ('.', 32), (\"'\", 20), ('l', 14), ('M', 10), ('la', 9), ('du', 9), ('Conseil', 8)]\n"
    }
   ],
   "source": [
    "model_types = [ModelType.Baseline, ModelType.Ours]\n",
    "column_values = ['LITERAL|COARSE', 'LITERAL|FINE']\n",
    "\n",
    "model_types_combinations = list(itertools.product([True, False], repeat=len(model_types)))\n",
    "for column_value in column_values:\n",
    "    for model_types_combination in model_types_combinations:\n",
    "        desired_prediction_status_dict = { model_type : model_types_combination[i] for i, model_type in enumerate(model_types) }\n",
    "        tokens_result = models_difference.get_tokens_per_result(\n",
    "            desired_prediction_status_dict,\n",
    "            column_value)\n",
    "\n",
    "        result_string = '------------------\\n'\n",
    "        for model_type, value in desired_prediction_status_dict.items():\n",
    "            result_string += 'correct' if value else 'wrong'\n",
    "            result_string += f' by {model_type.name.lower()}, '\n",
    "\n",
    "        result_string = result_string[:-2]\n",
    "\n",
    "        result_string += f' [{column_value}]\\n'\n",
    "        result_string += f'\\tTotal amount: {sum(tokens_result.values())}\\n'\n",
    "        result_string += f'\\tTop 10 most common: {tokens_result.most_common(10)}'\n",
    "        print(result_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('eval-env': conda)",
   "language": "python",
   "name": "python37564bitevalenvcondab07c5918277c4c33a244293f5160293b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}