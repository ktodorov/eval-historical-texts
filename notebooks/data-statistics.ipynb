{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import jellyfish\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ocr_path = os.path.join('..', 'data', 'ocr', 'pickles', 'combined_ocr.pickle')\n",
    "full_gs_path = os.path.join('..', 'data', 'ocr', 'pickles', 'combined_gs.pickle')\n",
    "data_path = os.path.join('..', 'data', 'ocr', 'full')\n",
    "\n",
    "start_position = 14\n",
    "\n",
    "def read_ocr_file(file_path: str):\n",
    "    with open(file_path, 'r', encoding='utf-8') as language_file:\n",
    "        text_data: List[str] = language_file.read().split('\\n')\n",
    "\n",
    "        return(text_data[1][start_position:], text_data[2][start_position:])\n",
    "\n",
    "def save_data_files():\n",
    "    ocr_aligned_lengths = []\n",
    "    gs_aligned_lengths = []\n",
    "    file_paths = []\n",
    "\n",
    "    for i, file_name in enumerate(os.listdir(data_path)):\n",
    "        file_paths.append(os.path.join(data_path, file_name))\n",
    "\n",
    "    number_of_files = len(file_paths)\n",
    "    file_data = []\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        print(f'{i}/{number_of_files}             \\r', end='')\n",
    "        file_data.append(read_ocr_file(file_path))\n",
    "        \n",
    "    ocr_file_data = [x[0] for x in file_data]\n",
    "    gs_file_data = [x[1] for x in file_data]\n",
    "    \n",
    "    with open(full_ocr_path, 'wb') as ocr_handle:\n",
    "        pickle.dump(ocr_file_data, ocr_handle, protocol=-1)\n",
    "    \n",
    "    with open(full_gs_path, 'wb') as gs_handle:\n",
    "        pickle.dump(gs_file_data, gs_handle, protocol=-1)\n",
    "        \n",
    "    return ocr_file_data, gs_file_data\n",
    "\n",
    "if not os.path.exists(full_ocr_path) or not os.path.exists(full_gs_path):\n",
    "    ocr_file_data, gs_file_data = save_data_files()\n",
    "else:\n",
    "    with open(full_ocr_path, 'rb') as ocr_handle:\n",
    "        ocr_file_data = pickle.load(ocr_handle)\n",
    "    \n",
    "    with open(full_gs_path, 'rb') as gs_handle:\n",
    "        gs_file_data = pickle.load(gs_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def plot_list_histogram(lst, title: str):\n",
    "    labels, values = zip(*Counter(lst).items())\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "\n",
    "    plt.bar(indexes, values, width)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def print_statistics(lst, title:str):\n",
    "    max_value = np.max(lst)\n",
    "    min_value = np.min(lst)\n",
    "    avg_value = np.mean(lst)\n",
    "    \n",
    "    print(f'{title}:\\nMAX: {max_value}\\nMIN: {min_value}\\nAVG: {avg_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_lengths = np.array([len(x) for x in ocr_file_data])\n",
    "gs_lengths = np.array([len(x) for x in gs_file_data])\n",
    "\n",
    "# indices = np.argsort(ocr_lengths)\n",
    "# print(indices)\n",
    "\n",
    "\n",
    "# print(ocr_file_data[0])\n",
    "# ocr_file_data_np = np.array(ocr_file_data, dtype=object)\n",
    "# gs_file_data_np = np.array(gs_file_data)[indices]\n",
    "\n",
    "\n",
    "# plot_list_histogram(ocr_lengths, 'OCR')\n",
    "# print_statistics(ocr_lengths, 'OCR')\n",
    "\n",
    "# plot_list_histogram(gs_lengths, 'GS')\n",
    "# print_statistics(gs_lengths, 'GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ocr_tokens_path = os.path.join('..', 'data', 'ocr', 'pickles', 'combined_ocr_tokens.pickle')\n",
    "full_gs_tokens_path = os.path.join('..', 'data', 'ocr', 'pickles', 'combined_gs_tokens.pickle')\n",
    "\n",
    "vocab_path = os.path.join('..', 'data', 'vocabularies', 'bert-base-cased-vocab.txt')\n",
    "tokenizer = BertWordPieceTokenizer(vocab_path, lowercase=False)\n",
    "\n",
    "if not os.path.exists(full_ocr_tokens_path) or not os.path.exists(full_gs_tokens_path):        \n",
    "    ocr_tokens = []\n",
    "    gs_tokens = []\n",
    "    for i in range(len(ocr_file_data)):\n",
    "        current_ids = tokenizer.encode(ocr_file_data[i]).ids\n",
    "        if len(current_ids) > 2000:\n",
    "            continue\n",
    "            \n",
    "        gs_ids = tokenizer.encode(gs_file_data[i]).ids\n",
    "        decoded_gs = tokenizer.decode(gs_ids)\n",
    "        if (decoded_gs.count('#') / len(decoded_gs)) > 0.15:\n",
    "            continue\n",
    "            \n",
    "        ocr_tokens.append(current_ids)\n",
    "        gs_tokens.append(gs_ids)\n",
    "    \n",
    "    with open(full_ocr_tokens_path, 'wb') as ocr_handle:\n",
    "        pickle.dump(ocr_tokens, ocr_handle, protocol=-1)\n",
    "    \n",
    "    with open(full_gs_tokens_path, 'wb') as gs_handle:\n",
    "        pickle.dump(gs_tokens, gs_handle, protocol=-1)\n",
    "else:\n",
    "    with open(full_ocr_tokens_path, 'rb') as ocr_handle:\n",
    "        ocr_tokens = pickle.load(ocr_handle)\n",
    "    \n",
    "    with open(full_gs_tokens_path, 'rb') as gs_handle:\n",
    "        gs_tokens = pickle.load(gs_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR - Less than 2000 length: 100.0\n",
      "GS  - Less than 2000 length: 100.0\n"
     ]
    }
   ],
   "source": [
    "ocr_tokens_lengths = np.array([len(x) for x in ocr_tokens])\n",
    "gs_tokens_lengths = np.array([len(x) for x in gs_tokens])\n",
    "\n",
    "# indices = np.argsort(ocr_tokens_lengths)\n",
    "\n",
    "# ocr_tokens_lengths = np.array(ocr_tokens_lengths)[indices]\n",
    "# gs_tokens_lengths = np.array(gs_tokens_lengths)[indices]\n",
    "# ocr_tokens = np.array(ocr_tokens)[indices]\n",
    "# gs_tokens = np.array(gs_tokens)[indices]\n",
    "\n",
    "print(f'OCR - Less than 2000 length: {len(ocr_tokens_lengths[ocr_tokens_lengths <= 2000]) / len(ocr_tokens_lengths) * 100}')\n",
    "print(f'GS  - Less than 2000 length: {len(gs_tokens_lengths[gs_tokens_lengths <= 2000]) / len(gs_tokens_lengths) * 100}')\n",
    "\n",
    "# plot_list_histogram(ocr_tokens_lengths, 'OCR - Tokens')\n",
    "# print_statistics(ocr_tokens_lengths, 'OCR - Tokens')\n",
    "\n",
    "# plot_list_histogram(gs_tokens_lengths, 'GS - Tokens')\n",
    "# print_statistics(gs_tokens_lengths, 'GS - Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity(list1: list, list2: list) -> float:\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def calculate_levenshtein_distance(string1: str, string2: str) -> int:\n",
    "    result = jellyfish.levenshtein_distance(string1, string2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics\n",
      "Saved metrics\n",
      "Saved metrics\n",
      "Saved metrics 0/966             \n",
      "Saved metrics 965/966             \n"
     ]
    }
   ],
   "source": [
    "def save_metrics_obj(token_pairs, decoded_pairs, jaccard_similarities, levenshtein_distances):\n",
    "    metrics_path = os.path.join('..', 'data', 'ocr', 'pickles', 'metrics.pickle')\n",
    "\n",
    "    metrics_obj = {\n",
    "        'token_pairs': token_pairs,\n",
    "        'decoded_pairs': decoded_pairs,\n",
    "        'jaccard_similarities': jaccard_similarities,\n",
    "        'levenshtein_distances': levenshtein_distances,\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, 'wb') as metrics_handle:\n",
    "        pickle.dump(metrics_obj, metrics_handle, protocol=-1)\n",
    "        \n",
    "    print('Saved metrics')\n",
    "\n",
    "def load_metrics_obj():\n",
    "    metrics_path = os.path.join('..', 'data', 'ocr', 'pickles', 'metrics.pickle')\n",
    "    if not os.path.exists(metrics_path):\n",
    "        return (None, None, None, None)\n",
    "\n",
    "    with open(metrics_path, 'rb') as metrics_handle:\n",
    "        metrics_obj = pickle.load(metrics_handle)\n",
    "        \n",
    "    return (metrics_obj['token_pairs'],\n",
    "            metrics_obj['decoded_pairs'],\n",
    "            metrics_obj['jaccard_similarities'],\n",
    "            metrics_obj['levenshtein_distances'])\n",
    "        \n",
    "token_pairs, decoded_pairs, jaccard_similarities, levenshtein_distances = load_metrics_obj()\n",
    "\n",
    "if not token_pairs:\n",
    "    token_pairs = [([tokenizer.id_to_token(x) for x in ocr_tokens[i]], [tokenizer.id_to_token(x) for x in gs_tokens[i]]) for i in range(len(ocr_tokens))]\n",
    "    save_metrics_obj(token_pairs, decoded_pairs, jaccard_similarities, levenshtein_distances)\n",
    "    \n",
    "if not decoded_pairs:\n",
    "    decoded_pairs = [(tokenizer.decode(ocr_tokens[i]), tokenizer.decode(gs_tokens[i])) for i in range(len(ocr_tokens))]\n",
    "    save_metrics_obj(token_pairs, decoded_pairs, jaccard_similarities, levenshtein_distances)\n",
    "    \n",
    "all_pairs = len(token_pairs)\n",
    "if not jaccard_similarities:\n",
    "    jaccard_similarities = []\n",
    "    for i, token_pair in enumerate(token_pairs):\n",
    "        jaccard_similarities.append(calculate_jaccard_similarity(token_pair[0], token_pair[1]))\n",
    "    \n",
    "    save_metrics_obj(token_pairs, decoded_pairs, jaccard_similarities, levenshtein_distances)\n",
    "    \n",
    "if not levenshtein_distances:\n",
    "    levenshtein_distances = []\n",
    "    \n",
    "if len(levenshtein_distances) < all_pairs:\n",
    "    for i, decoded_pair in enumerate(decoded_pairs):\n",
    "        if i < len(levenshtein_distances):\n",
    "            continue\n",
    "            \n",
    "        print(f'LEVENSHTEIN - {i}/{all_pairs}             \\r', end='')\n",
    "        levenshtein_distances.append(calculate_levenshtein_distance(decoded_pair[0], decoded_pair[1]))\n",
    "        \n",
    "        if i % 5000 == 0:\n",
    "            save_metrics_obj(token_pairs, decoded_pairs, jaccard_similarities, levenshtein_distances)\n",
    "    \n",
    "    save_metrics_obj(token_pairs, decoded_pairs, jaccard_similarities, levenshtein_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_string(string):\n",
    "    result = string.replace(' ##', '')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 206.86\n",
      "Jaccard similarity:     0.82\n"
     ]
    }
   ],
   "source": [
    "# counts = np.array([(x[1].count('#')/len(x[1])) for x in decoded_pairs])\n",
    "\n",
    "print(f'Levenshtein distance: {np.mean(levenshtein_distances):5.2f}')\n",
    "print(f'Jaccard similarity: {np.mean(jaccard_similarities):8.2f}')\n",
    "\n",
    "train_pickle_path = os.path.join('..', 'data', 'ocr', 'pickles', 'train_pairs.pickle')\n",
    "eval_pickle_path = os.path.join('..', 'data', 'ocr', 'pickles', 'eval_pairs.pickle')\n",
    "\n",
    "if not os.path.exists(train_pickle_path) or not os.path.exists(eval_pickle_path):    \n",
    "    eval_indices = random.sample(range(len(token_pairs)), int(0.2 * len(token_pairs)))\n",
    "\n",
    "    train_pairs = []\n",
    "    eval_pairs = []\n",
    "    for i, token_pair in enumerate(token_pairs):\n",
    "        if i in eval_indices:\n",
    "            eval_pairs.append(token_pair)\n",
    "        else:\n",
    "            train_pairs.append(token_pair)\n",
    "\n",
    "    with open(train_pickle_path, 'wb') as train_handle:\n",
    "        pickle.dump(train_pairs, train_handle, protocol=-1)\n",
    "    \n",
    "    with open(eval_pickle_path, 'wb') as eval_handle:\n",
    "        pickle.dump(eval_pairs, eval_handle, protocol=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eval-env]",
   "language": "python",
   "name": "conda-env-eval-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
