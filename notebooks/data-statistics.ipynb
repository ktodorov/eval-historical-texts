{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import jellyfish\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ocr_path = os.path.join('..', 'data', 'post-ocr-correction', 'pickles', 'combined_ocr.pickle')\n",
    "full_gs_path = os.path.join('..', 'data', 'post-ocr-correction', 'pickles', 'combined_gs.pickle')\n",
    "data_path = os.path.join('..', 'data', 'post-ocr-correction', 'full')\n",
    "\n",
    "if not os.path.exists(full_ocr_path) or not os.path.exists(full_gs_path):\n",
    "    raise Exception('data not found')\n",
    "else:\n",
    "    with open(full_ocr_path, 'rb') as ocr_handle:\n",
    "        ocr_file_data = pickle.load(ocr_handle)\n",
    "    \n",
    "    with open(full_gs_path, 'rb') as gs_handle:\n",
    "        gs_file_data = pickle.load(gs_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def plot_list_histogram(lst, title: str):\n",
    "    labels, values = zip(*Counter(lst).items())\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "\n",
    "    plt.bar(indexes, values, width)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def print_statistics(lst, title:str):\n",
    "    max_value = np.max(lst)\n",
    "    min_value = np.min(lst)\n",
    "    avg_value = np.mean(lst)\n",
    "    \n",
    "    print(f'{title}:\\nMAX: {max_value}\\nMIN: {min_value}\\nAVG: {avg_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_lengths = np.array([len(x) for x in ocr_file_data])\n",
    "gs_lengths = np.array([len(x) for x in gs_file_data])\n",
    "\n",
    "# indices = np.argsort(ocr_lengths)\n",
    "# print(indices)\n",
    "\n",
    "\n",
    "# print(ocr_file_data[0])\n",
    "# ocr_file_data_np = np.array(ocr_file_data, dtype=object)\n",
    "# gs_file_data_np = np.array(gs_file_data)[indices]\n",
    "\n",
    "\n",
    "# plot_list_histogram(ocr_lengths, 'OCR')\n",
    "# print_statistics(ocr_lengths, 'OCR')\n",
    "\n",
    "# plot_list_histogram(gs_lengths, 'GS')\n",
    "# print_statistics(gs_lengths, 'GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ocr_tokens_path = os.path.join('..', 'data', 'post-ocr-correction', 'pickles', 'combined_ocr_tokens.pickle')\n",
    "full_gs_tokens_path = os.path.join('..', 'data', 'post-ocr-correction', 'pickles', 'combined_gs_tokens.pickle')\n",
    "\n",
    "vocab_path = os.path.join('..', 'data', 'vocabularies', 'bert-base-cased-vocab.txt')\n",
    "tokenizer = BertWordPieceTokenizer(vocab_path, lowercase=False)\n",
    "\n",
    "if not os.path.exists(full_ocr_tokens_path) or not os.path.exists(full_gs_tokens_path):        \n",
    "    raise Exception('Data not found')\n",
    "else:\n",
    "    with open(full_ocr_tokens_path, 'rb') as ocr_handle:\n",
    "        ocr_tokens = pickle.load(ocr_handle)\n",
    "    \n",
    "    with open(full_gs_tokens_path, 'rb') as gs_handle:\n",
    "        gs_tokens = pickle.load(gs_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_tokens_lengths = np.array([len(x) for x in ocr_tokens])\n",
    "gs_tokens_lengths = np.array([len(x) for x in gs_tokens])\n",
    "\n",
    "# indices = np.argsort(ocr_tokens_lengths)\n",
    "\n",
    "# ocr_tokens_lengths = np.array(ocr_tokens_lengths)[indices]\n",
    "# gs_tokens_lengths = np.array(gs_tokens_lengths)[indices]\n",
    "# ocr_tokens = np.array(ocr_tokens)[indices]\n",
    "# gs_tokens = np.array(gs_tokens)[indices]\n",
    "\n",
    "# print(f'OCR - Less than 2000 length: {len(ocr_tokens_lengths[ocr_tokens_lengths <= 2000]) / len(ocr_tokens_lengths) * 100}')\n",
    "# print(f'GS  - Less than 2000 length: {len(gs_tokens_lengths[gs_tokens_lengths <= 2000]) / len(gs_tokens_lengths) * 100}')\n",
    "\n",
    "# plot_list_histogram(ocr_tokens_lengths, 'OCR - Tokens')\n",
    "# print_statistics(ocr_tokens_lengths, 'OCR - Tokens')\n",
    "\n",
    "# plot_list_histogram(gs_tokens_lengths, 'GS - Tokens')\n",
    "# print_statistics(gs_tokens_lengths, 'GS - Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity(list1: list, list2: list) -> float:\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def calculate_levenshtein_distance(string1: str, string2: str) -> int:\n",
    "    result = jellyfish.levenshtein_distance(string1, string2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics_obj():\n",
    "    metrics_path = os.path.join('..', 'data', 'post-ocr-correction', 'pickles', 'metrics.pickle')\n",
    "    if not os.path.exists(metrics_path):\n",
    "        return (None, None, None, None)\n",
    "\n",
    "    with open(metrics_path, 'rb') as metrics_handle:\n",
    "        metrics_obj = pickle.load(metrics_handle)\n",
    "        \n",
    "    return (metrics_obj['token_pairs'],\n",
    "            metrics_obj['decoded_pairs'],\n",
    "            metrics_obj['jaccard_similarities'],\n",
    "            metrics_obj['levenshtein_distances'])\n",
    "        \n",
    "token_pairs, decoded_pairs, jaccard_similarities, levenshtein_distances = load_metrics_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_string(string):\n",
    "    result = string.replace(' ##', '')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_levenshtein_distances = []\n",
    "for i in range(len(decoded_pairs)):\n",
    "    max_len = max(len(decoded_pairs[i][0]), len(decoded_pairs[i][1]))\n",
    "    normalized_levenshtein_distances.append(float(levenshtein_distances[i]) / max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2697774965579539\n",
      "0.003902610744621192\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# bins = np.arange(0,1, 0.001)\n",
    "# plt.hist(normalized_levenshtein_distances, bins = bins, edgecolor='none')\n",
    "# plt.show()\n",
    "\n",
    "np_norm_ld = np.array(normalized_levenshtein_distances)\n",
    "# print(np_norm_ld[np_norm_ld > 0.3])# / len(np_norm_ld))\n",
    "print(len(np_norm_ld[np_norm_ld < 0.001]) / len(np_norm_ld))\n",
    "\n",
    "# for i, ld in enumerate(normalized_levenshtein_distances):\n",
    "#     if ld > 0.3 and ld < 0.33 and i > 697:\n",
    "#         print(i)\n",
    "#         print(decode_string(decoded_pairs[i][0]))\n",
    "#         print('-----------------------')\n",
    "#         print(decode_string(decoded_pairs[i][1]))\n",
    "#         break\n",
    "        \n",
    "\n",
    "print(np.mean(normalized_levenshtein_distances))\n",
    "\n",
    "print(len(gs_lengths[gs_lengths > 2000]))\n",
    "# bins = np.arange(0, max(gs_lengths), 10)\n",
    "# plt.hist(gs_lengths, bins = bins, edgecolor='none')\n",
    "# plt.show()\n",
    "\n",
    "# print(np.argmax(normalized_levenshtein_distances))\n",
    "# print(levenshtein_distances[647])\n",
    "# print(decoded_pairs[647][0])\n",
    "# print('-----------------------------')\n",
    "# print(decoded_pairs[647][1])\n",
    "\n",
    "# lvd_np = np.array(levenshtein_distances)\n",
    "# print(len(lvd_np[lvd_np > 1000]) / len(lvd_np))\n",
    "# jcd_np = np.array(jaccard_similarities)\n",
    "# print(len(jcd_np[jcd_np < 0.4]) / len(jcd_np))\n",
    "# print(jaccard_similarities[np.argmax(lvd_np)])\n",
    "# print(decoded_pairs[np.argmax(lvd_np)][0])\n",
    "# print('-------------------------------------------------')\n",
    "# print(decoded_pairs[np.argmax(lvd_np)][1])\n",
    "\n",
    "# print(f'Levenshtein distance: {np.mean(levenshtein_distances):5.2f}')\n",
    "# print(f'Jaccard similarity: {np.mean(jaccard_similarities):8.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.3\n",
    "LENGTH_THRESHOLD = 5000\n",
    "\n",
    "new_indices = []\n",
    "for i, nm_ld in enumerate(normalized_levenshtein_distances):\n",
    "    if nm_ld > THRESHOLD:\n",
    "        continue\n",
    "        \n",
    "    if len(decoded_pairs[i][0]) > LENGTH_THRESHOLD or len(decoded_pairs[i][1]) > LENGTH_THRESHOLD:\n",
    "        continue\n",
    "        \n",
    "    new_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pickle_path = os.path.join('..', 'data', 'post-ocr-correction', 'pickles', 'train_pairs.pickle')\n",
    "eval_pickle_path = os.path.join('..', 'data', 'post-ocr-correction', 'pickles', 'eval_pairs.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "decoded_ocr_lengths = [len(x[0]) for x in decoded_pairs]\n",
    "decoded_gs_lengths = [len(x[1]) for x in decoded_pairs]\n",
    "\n",
    "np_ocr_lengths = np.array(decoded_ocr_lengths)\n",
    "np_gs_lengths = np.array(decoded_gs_lengths)\n",
    "print(len(np_ocr_lengths[np_ocr_lengths > 5000]) / len(np_ocr_lengths))\n",
    "print(len(np_gs_lengths[np_gs_lengths > 5000]) / len(np_gs_lengths))\n",
    "\n",
    "# plt.scatter(decoded_ocr_lengths, decoded_gs_lengths, s=.5)\n",
    "# ident = [0.0, max(max(decoded_ocr_lengths), max(decoded_gs_lengths))]\n",
    "# plt.plot(ident,ident, c='r', linewidth=1, linestyle='--')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gland Beware of a Great MISHAP.-x 744 renncfs in o\n",
      "---------------------------\n",
      "gland Beware of a Great MISHAP. ##################\n"
     ]
    }
   ],
   "source": [
    "print(decoded_pairs[new_indices[1050]][0])\n",
    "print('---------------------------')\n",
    "print(decoded_pairs[new_indices[1050]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eval-env] *",
   "language": "python",
   "name": "conda-env-eval-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
