# Transfer Learning for historical texts: Evaluating the impact of OCR and language change on the transferability of language models

_Supervisors: **Giovanni Colavizza** and **Daniel van Strien** (British Library)_

Pre-trained language models utilizing deep learning have led to state-of-the-art results in many NLP tasks. Compared to Word2Vec and other word embedding models, new deep learning language models, such as ULMFiT, ELMo and BERT are able to capture richer representations of language features. A major benefit of these models is that they can be trained on large corpora without supervision. These pre-trained language models have subsequently been made available for others to use. This has had the to a reduction in the amount of trained labels required for NLP tasks and more efficient training (both in time and cost) and increased accuracy of models.

There are a number of potential challenges in applying these models in a digital humanities context. Firstly, pre-trained language models are trained on modern text, largely gathered from the Web. Secondly, these pre-trained language models are not trained on text produced as a result of OCR. This project aims to explore the impact of these two challenges. More specifically it would be suggested that the student explores the use of ULMFiT and BERT on classification and Named Entity Recognition tasks. This project would explore how well these models perform on historic texts with and without the use of fine tuning these models. Other NLP tasks and models could be explored depending on the studentsâ€™ interest. There are a range of outputs that could emerge from the project, including publications offering initial guidance for other researchers on the most effective approach to utilizing these models for historic OCR text. The project uses data and benefits from experts from the Living with Machines project, based the The Alan Turing Institute and the British Library.
